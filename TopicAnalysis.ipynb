{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take the last 24 hours of a customer's calls and provide the major topics\n",
    "import json\n",
    "import os\n",
    "\n",
    "with open('transcription.json') as transcription_file:\n",
    "    data = json.load(transcription_file)\n",
    "    \n",
    "#print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(data['utterances']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Okay and now we have the three of us on at least on the telephone. Yeah, and I can hear you both. Yeah. Okay, um, so I wanted to I share my screen and give you some insight to that database says everybody awake enough to do that or not. Yeah, if you're in the morning people are not this is a fine our it's both I start drag and about ### or seven o'clock. Oh, you know what okay are you able to hear me better now, yeah and and black I don't know I can't hear you at all and it's like okay. Okay, I\n"
     ]
    }
   ],
   "source": [
    "#testing: get full transcription from the transcript\n",
    "\n",
    "def get_words(transcription: dict) -> str:\n",
    "    \n",
    "    consolidated_transcription = \"\"\n",
    "    for utterances in transcription[\"utterances\"]:\n",
    "        for i in utterances[\"events\"]:\n",
    "            word = i[\"word\"]\n",
    "            #print(word)\n",
    "            consolidated_transcription = consolidated_transcription + \" \" + word\n",
    "            \n",
    "    return consolidated_transcription\n",
    "\n",
    "print(get_words(data)[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Okay', 'and', 'now', 'we', 'have', 'the', 'three', 'of', 'us', 'on', 'at', 'least', 'on', 'the', 'telephone.', 'Yeah,', 'and', 'I', 'can', 'hear', 'you', 'both.', 'Yeah.', 'Okay,', 'um,', 'so', 'I', 'wanted', 'to', 'I', 'share', 'my', 'screen', 'and', 'give', 'you', 'some', 'insight', 'to', 'that', 'database', 'says', 'everybody', 'awake', 'enough', 'to', 'do', 'that', 'or', 'not.', 'Yeah,', 'if', \"you're\", 'in', 'the', 'morning', 'people', 'are', 'not', 'this', 'is', 'a', 'fine', 'our', \"it's\", 'both', 'I', 'start', 'drag', 'and', 'about', '###', 'or', 'seven', \"o'clock.\", 'Oh,', 'you', 'know', 'what', 'okay', 'are', 'you', 'able', 'to', 'hear', 'me', 'better', 'now,', 'yeah', 'and', 'and', 'black', 'I', \"don't\", 'know', 'I', \"can't\", 'hear', 'you', 'at']\n"
     ]
    }
   ],
   "source": [
    "#get words from the script\n",
    "def get_words(transcription: dict) -> str:\n",
    "    \n",
    "    words_list = []\n",
    "    consolidated_transcription = \"\"\n",
    "    for utterances in transcription[\"utterances\"]:\n",
    "        for i in utterances[\"events\"]:\n",
    "            word = i[\"word\"]\n",
    "            #print(word)\n",
    "            words_list.append(word)\n",
    "            \n",
    "    return words_list\n",
    "\n",
    "print(get_words(data)[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7179\n",
      "['okay', 'and', 'now', 'we', 'have', 'the', 'three', 'of', 'us', 'on', 'at', 'least', 'on', 'the', 'telephone', 'yeah', 'and', 'i', 'can', 'hear', 'you', 'both', 'yeah', 'okay', 'um', 'so', 'i', 'wanted', 'to', 'i', 'share', 'my', 'screen', 'and', 'give', 'you', 'some', 'insight', 'to', 'that', 'database', 'says', 'everybody', 'awake', 'enough', 'to', 'do', 'that', 'or', 'not', 'yeah', 'if', \"you're\", 'in', 'the', 'morning', 'people', 'are', 'not', 'this', 'is', 'a', 'fine', 'our', \"it's\", 'both', 'i', 'start', 'drag', 'and', 'about', '###', 'or', 'seven', \"o'clock\", 'oh', 'you', 'know', 'what', 'okay', 'are', 'you', 'able', 'to', 'hear', 'me', 'better', 'now', 'yeah', 'and', 'and', 'black', 'i', \"don't\", 'know', 'i', \"can't\", 'hear', 'you', 'at']\n"
     ]
    }
   ],
   "source": [
    "#Data cleansing: remove punctuation/ lower casing\n",
    "import re\n",
    "\n",
    "words_data = get_words(data)\n",
    "\n",
    "#remove puntuation\n",
    "words_data = [re.sub('[,\\.!?]', '', x) for x in words_data]\n",
    "\n",
    "#convert to lowercase\n",
    "words_data = [x.lower() for x in words_data]\n",
    "\n",
    "print(len(words_data))\n",
    "print(words_data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Warot\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3056\n",
      "['okay', 'three', 'least', 'telephone', 'yeah', 'hear', 'yeah', 'okay', 'um', 'wanted', 'share', 'screen', 'give', 'insight', 'database', 'says', 'everybody', 'awake', 'enough', 'yeah', 'morning', 'people', 'fine', 'start', 'drag', 'seven', \"o'clock\", 'oh', 'know', 'okay', 'able', 'hear', 'better', 'yeah', 'black', 'know', \"can't\", 'hear', 'like', 'okay', 'okay', 'okay', 'anyway', \"that's\", 'wireless', 'work', 'wanted', 'time', 'trying', 'get', 'slack', 'going', 'back', 'um', 'cella', 'using', 'using', 'usb', 'ports', 'bank', 'space', \"there's\", 'four', 'right', 'yes', 'monitor', 'center', 'yet', 'send', 'monitors', 'connectors', 'thanks', 'patient', 'one', 'monitor', 'get', 'doctor', 'like', 'micro', 'something', 'call', 'forgot', 'different', 'type', 'sports', 'could', 'plus', 'monitoring', 'plugged', 'back', 'think', 'station', 'know', 'sounds', 'like', 'odd', 'question', 'plugged', 'usb', 'port']\n"
     ]
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'us', '##', '###','####', '#####', '########'])\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    \n",
    "    filtered_words = []\n",
    "    for word in texts:\n",
    "        if word not in stop_words:\n",
    "            filtered_words.append(word)\n",
    "    \n",
    "    return filtered_words\n",
    "\n",
    "words_data = remove_stopwords(words_data)\n",
    "print(len(words_data))\n",
    "print(words_data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "965\n"
     ]
    }
   ],
   "source": [
    "#check total unique words\n",
    "a_set = set(words_data)\n",
    "\n",
    "print(len(a_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          word  counts\n",
      "0          50%       1\n",
      "1          90%       1\n",
      "2         able       5\n",
      "3          abs       1\n",
      "4      academy       3\n",
      "..         ...     ...\n",
      "957  yesterday       1\n",
      "958        yet       3\n",
      "959    younger       2\n",
      "960       zero       1\n",
      "961        zip       1\n",
      "\n",
      "[962 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#create dataframe, group and count identical word\n",
    "\n",
    "df = pd.DataFrame(words_data, columns = ['word'])\n",
    "df_words = df.groupby(['words']).size().reset_index(name='counts')\n",
    "print(df_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
